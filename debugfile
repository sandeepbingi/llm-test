import argparse
import os
import boto3
import pickle
import pandas as pd
import json
from prophet import Prophet
from io import BytesIO
import logging

# Setup logging
logging.basicConfig(level=logging.DEBUG, format="%(asctime)s [%(levelname)s] %(message)s")
log = logging.getLogger(__name__)

def read_csv_from_s3(bucket, key):
    log.debug(f"Reading CSV from s3://{bucket}/{key}")
    s3 = boto3.client("s3")
    obj = s3.get_object(Bucket=bucket, Key=key)
    return pd.read_csv(BytesIO(obj["Body"].read()))

def load_processed_files(bucket, key):
    log.debug(f"Loading processed file list from s3://{bucket}/{key}")
    s3 = boto3.client("s3")
    try:
        obj = s3.get_object(Bucket=bucket, Key=key)
        return set(pickle.loads(obj["Body"].read()))
    except s3.exceptions.NoSuchKey:
        log.warning("Tracking file does not exist yet. Starting fresh.")
        return set()
    except Exception as e:
        log.error(f"Error loading processed files: {e}")
        raise

def save_processed_files(bucket, key, processed_files):
    log.debug(f"Saving processed file list to s3://{bucket}/{key}")
    s3 = boto3.client("s3")
    s3.put_object(Bucket=bucket, Key=key, Body=pickle.dumps(processed_files))

def load_hyperparameters(bucket, key):
    log.debug(f"Loading hyperparameters from s3://{bucket}/{key}")
    s3 = boto3.client("s3")
    obj = s3.get_object(Bucket=bucket, Key=key)
    return json.load(obj["Body"])

def train_model(channel, df, hyperparams, output_path):
    log.debug(f"Training model for channel: {channel}")
    df = df.rename(columns={"timestamp": "ds", "count": "y"})
    df["y"] = df["y"].fillna(0)
    params = hyperparams.get(channel, {})
    model = Prophet(**{k: v for k, v in params.items() if "fourier_order" not in k})

    if "hourly_fourier_order" in params:
        model.add_seasonality("hourly", 24, params["hourly_fourier_order"])
    if "weekly_fourier_order" in params:
        model.add_seasonality("weekly", 7, params["weekly_fourier_order"])
    if "monthly_fourier_order" in params:
        model.add_seasonality("monthly", 30, params["monthly_fourier_order"])

    model.fit(df)
    log.debug(f"Model trained. Saving to {output_path}")
    with open(output_path, "wb") as f:
        pickle.dump({"model": model, "data": df}, f)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--bucket", type=str)
    parser.add_argument("--data_prefix", type=str, default="")
    parser.add_argument("--channel", type=str)
    parser.add_argument("--tracking_key", type=str)
    parser.add_argument("--hyperparams_key", type=str)
    parser.add_argument("--model_key", type=str)
    args = parser.parse_args()

    log.debug(f"Arguments: {args}")

    required_keys = {
        "tracking_key": args.tracking_key,
        "hyperparams_key": args.hyperparams_key,
        "model_key": args.model_key
    }
    for name, val in required_keys.items():
        if not val or not val.strip():
            log.error(f"Missing or empty argument: {name}")
            raise ValueError(f"Missing or empty argument: {name}")

    s3 = boto3.client("s3")
    prefix = args.data_prefix if args.data_prefix else ""
    log.debug(f"Listing objects in s3://{args.bucket}/{prefix}")
    files = s3.list_objects_v2(Bucket=args.bucket, Prefix=prefix).get("Contents", [])
    file_keys = [f["Key"] for f in files if f["Key"].endswith(".csv")]
    log.debug(f"Found {len(file_keys)} CSV files")

    processed_files = load_processed_files(args.bucket, args.tracking_key)
    new_files = set(file_keys) - processed_files
    log.debug(f"New files to process: {new_files}")

    if not new_files:
        log.info("No new files found. Exiting.")
        exit(0)

    df_list = []
    for key in new_files:
        log.debug(f"Processing new file: {key}")
        df = read_csv_from_s3(args.bucket, key)
        df_long = df.melt(id_vars=["Channel"], var_name="timestamp", value_name="count")
        df_long["timestamp"] = pd.to_datetime(df_long["timestamp"], errors="coerce")
        df_list.append(df_long)

    data = pd.concat(df_list, ignore_index=True)
    data = data[data["Channel"] == args.channel]

    if data.empty:
        log.warning(f"No data found for channel: {args.channel}. Exiting.")
        exit(0)

    hyperparams = load_hyperparameters(args.bucket, args.hyperparams_key)

    os.makedirs("/opt/ml/model", exist_ok=True)
    model_path = "/opt/ml/model/model.pkl"
    train_model(args.channel, data, hyperparams, model_path)

    processed_files.update(new_files)
    save_processed_files(args.bucket, args.tracking_key, processed_files)
    log.info("Training complete and model saved.")
